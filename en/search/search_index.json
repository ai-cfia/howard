{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Howard project","text":""},{"location":"#howard-cloud-infrastructure-of-the-canadian-food-inspection-agency-cfia-acia-cfia-ai-lab","title":"Howard: Cloud infrastructure of the Canadian Food Inspection Agency (CFIA) ACIA-CFIA ai-Lab","text":""},{"location":"#about-the-project","title":"About the project","text":"<p>The Howard project is named after Luke Howard, FRS, a notable British manufacturing chemist and amateur meteorologist known as \"The Godfather of Clouds\". His work laid foundational concepts in meteorology, including a nomenclature system for clouds introduced in 1802. Inspired by his innovation and legacy in categorizing the elements, our project aims to effectively manage and orchestrate the cloud-based infrastructure for the Canadian Food Inspection Agency (CFIA) ai-lab.</p> <p>Howard is essentially the backbone that supports CFIA's ai-lab Kubernetes environment, where key applications such as Nachet, Finesse, and Louis are deployed and managed dynamically. This infrastructure emphasizes robustness, security, and efficiency to handle the critical workload involved in food inspection and safety.</p>"},{"location":"#technology-stack-and-tools","title":"Technology stack and tools","text":"<p>The Howard infrastructure leverages a comprehensive suite of tools designed to provide a resilient, secure, and scalable environment:</p>"},{"location":"#cloud-providers","title":"Cloud providers","text":"<ul> <li>Initially hosted on Google Cloud, the infrastructure has transitioned to Azure.</li> </ul>"},{"location":"#container-orchestration","title":"Container orchestration","text":"<ul> <li>Kubernetes: Orchestrates container deployment, scaling, and management.</li> </ul>"},{"location":"#gitops","title":"GitOps","text":"<ul> <li>ArgoCD: Used for continuous delivery, managing Kubernetes resources in a declarative way through Git repositories.</li> </ul>"},{"location":"#monitoring-and-security","title":"Monitoring and security","text":"<ul> <li>Grafana: Visualization and analytics software.</li> <li>Kube-Prometheus-Stack: Comprehensive Kubernetes cluster monitoring with Prometheus.</li> <li>Grafana Tempo Distributed tracing of our applications.</li> <li>Grafana Loki: Logging and aggregation system.</li> <li>Grafana Alloy: Allows the collection and transmission of OpenTelemetry   data from our applications.</li> <li>Falco: Open-source runtime security tool.</li> <li>Trivy: Vulnerability scanner for containers.</li> <li>Oneuptime: Monitoring tool for real-time performance and security   insights.</li> </ul>"},{"location":"#networking","title":"Networking","text":"<ul> <li>Vouch-Proxy: Authentication proxy.</li> <li>Nginx Ingress: Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.</li> <li>Istio: Service mesh that provides a secure interface for inter-service   communication.</li> </ul>"},{"location":"#secrets-management","title":"Secrets management","text":"<ul> <li>HashiCorp Vault: Secures, stores, and tightly controls access to tokens, passwords, certificates, and other secrets.</li> </ul>"},{"location":"#cloud-infrastructure-management","title":"Cloud infrastructure management","text":"<ul> <li>Terraform: Open-source infrastructure as code software tool that allows managing service life cycle in cloud providers declaratively.</li> <li>Ansible: Automation tool for configuring and managing computers.</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#terraform-deployment","title":"Terraform deployment","text":"<p>Current configuration is hosting a kubernetes cluster on Azure (AKS). We have an Azure Devops pipeline <code>apply-terraform.yml</code> that applies terraform's resources that are created on our Azure's subscription. The state is then saved to a blob storage in Azure.</p>"},{"location":"#kubectl-configuration","title":"Kubectl configuration","text":"<p>Assuming you have Azure's CLI and kubelogin plugin installed, here is how you can locally fetch the kube config :</p> <pre><code>az login\naz account set --subscription xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\naz aks get-credentials --resource-group resource-group-name --name aks-name --overwrite-existing\nkubelogin convert-kubeconfig -l azurecli\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>https://ai-cfia.github.io/howard/en/</p>"},{"location":"adr/","title":"Architectural decision records (ADR)","text":"<ul> <li>Infrastructure</li> <li>GitOps</li> <li>Secret management</li> <li>Infrastructure as code (IaC)</li> <li>Containers</li> <li>Authentication management</li> <li>Networking</li> <li>Security</li> </ul>"},{"location":"adr/#more-configuration-implementation","title":"More configuration / implementation","text":"<ul> <li>Ansible</li> </ul>"},{"location":"ansible/","title":"Ansible playbooks","text":""},{"location":"ansible/#executive-summary","title":"Executive Summary","text":"<p>As part of our tasks, we aimed to create virtual machines (VMs) to facilitate the work of developers. The following needs were mentioned:</p> <ul> <li> <p>Testing applications before requesting IT to install the software on our personal laptops</p> </li> <li> <p>Facilitating the work of certain developers (e.g., having CLIs already installed and configured)</p> </li> </ul> <p>To set up virtual machines in the cloud, using Ansible was very useful to enable scaling in case we need more VMs in the future.</p>"},{"location":"ansible/#glossary","title":"Glossary","text":"<p>Ansible: Ansible is an open-source software tool for IT automation. It automates the provisioning, configuration, and deployment of servers and applications. Ansible is agentless, meaning it does not require any software to be installed on the machines it manages. It uses SSH to connect to machines and execute commands.</p> <p>Virtual machine (VM): A virtual machine is a software that simulates a complete computer, with its own operating system and applications, and runs on a physical computer. In short, it is a virtual computer inside a physical computer.</p>"},{"location":"ansible/#diagrams","title":"Diagrams","text":""},{"location":"ansible/#references","title":"References","text":"<p>Kubernetes</p> <p>Pod</p> <p>HA</p> <p>Load balancer</p>"},{"location":"auth-workflow/","title":"Vouch-Proxy Documentation","text":""},{"location":"auth-workflow/#overview","title":"Overview","text":"<p>Vouch-Proxy is an authentication and authorization solution that acts as a companion to our Nginx ingress controller. It's designed to authenticate users against an OpenID Connect provider (OIDC) and then pass those validated credentials to our web application.</p>"},{"location":"auth-workflow/#integration-with-azure-active-directory","title":"Integration with Azure Active Directory","text":"<p>Vouch-Proxy can be configured to authenticate users via Azure Active Directory (Azure AD), leveraging Azure's App Registrations to authenticate users from a specific Azure AD tenant. This setup involves:</p> <ul> <li>Creating an App Registration in Azure AD.</li> <li>Configuring the redirect URIs for the App Registration to   <code>&lt;https://vouch.inspection.alpha.canada.ca/auth&gt;</code></li> <li>Using the App Registration's details (client ID, client secret, tenantID) in   Vouch-Proxy's configuration :</li> </ul> <pre><code>  client_id: &lt;id&gt;\n  client_secret: &lt;secret&gt;\n  auth_url: https://login.microsoftonline.com/&lt;tenantID&gt;/oauth2/v2.0/authorize\n  token_url: https://login.microsoftonline.com/&lt;tenantID&gt;/oauth2/v2.0/token\n</code></pre> <p>When a user attempts to access a protected resource, they are redirected to Azure AD to log in. Once authenticated, Azure AD redirects back to Vouch-Proxy, which then validates the user's session and forwards the authentication details to the Nginx ingress controller.</p>"},{"location":"auth-workflow/#nginx-ingress-annotations-for-authentication","title":"Nginx Ingress Annotations for Authentication","text":"<p>To protect an application using Vouch-Proxy, you can configure Nginx ingress resources with specific annotations. These annotations instruct the Nginx controller to consult Vouch-Proxy for authentication before granting access to the application. Here\u2019s how to configure these annotations for an app with the ingress hostname <code>vouch.inspection.alpha.canada.ca</code>:</p> <pre><code>annotations:\n  nginx.ingress.kubernetes.io/auth-signin: \"https://vouch.inspection.alpha.canada.ca/login?url=$scheme://$http_host$request_uri&amp;vouch-failcount=$auth_resp_failcount&amp;X-Vouch-Token=$auth_resp_jwt&amp;error=$auth_resp_err\"\n  nginx.ingress.kubernetes.io/auth-url: https://vouch.inspection.alpha.canada.ca/validate\n  nginx.ingress.kubernetes.io/auth-snippet: |\n    # these return values are used by the @error401 call\n    auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;\n    auth_request_set $auth_resp_err $upstream_http_x_vouch_err;\n    auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;\n</code></pre>"},{"location":"auth-workflow/#authentication-flow","title":"Authentication Flow","text":"<ol> <li>A user requests access to an application protected by Vouch-Proxy.</li> <li>The Nginx ingress controller intercepts the request and queries Vouch-Proxy    to validate the user's session.</li> <li>If the user is not authenticated, they are redirected to the Azure AD login    page.</li> <li>After successful authentication, the user is redirected back to Vouch-Proxy,</li> <li>Vouch-proxy then sets a cookie in the user's browser and redirects the user    back to the original application, passing along any specified user details.</li> </ol>"},{"location":"configuration/","title":"Configuration / implementation of ADRs inside the Howard project","text":"<ul> <li>Infrastructure</li> <li>GitOps (coming soon...)</li> <li>Secret management</li> <li>Infrastructure as code (IaC)</li> <li>Containers</li> <li>Authentication management</li> <li>Networking</li> <li>Security (coming soon...)</li> </ul>"},{"location":"configuration/#more-configuration-implementation","title":"More configuration / implementation","text":"<ul> <li>Ansible</li> <li>Multi layered application</li> </ul>"},{"location":"generic-architecture/","title":"Generic architecture","text":""},{"location":"generic-architecture/#executive-summary","title":"Executive summary","text":"<p>These diagrams in question provide a visual representation of the planned infrastructure strategy for the ai-lab at the Canadian Food Inspection Agency (CFIA). The reason for this design is to cater to the needs of users spread across the vast expanse of Canada, including major user groups in canada central and canada east. By establishing two clusters in the Central and Eastern geographic regions of Canada, the CFIA aims to deliver optimal service to all users regardless of their location. This approach not only ensures high availability (HA) by mitigating the risk of service disruption due to regional outages but also maintains a one-to-one redundancy of all services, which is crucial for disaster recovery and uninterrupted operations. The strategic placement of these clusters allows for efficient data replication and swift failover processes, thereby providing a robust and reliable infrastructure for the agency's critica operations.</p>"},{"location":"generic-architecture/#glossary","title":"Glossary","text":"<p>Kubernetes: Kubernetes is an open-source platform designed to automate deploying, scaling, and operating application containers. It groups containers that make up an application into logical units for easy management and discovery. Kubernetes provides tools for orchestrating distributed systems at scale.</p> <p>Pod: In the context of Kubernetes, a Pod is the smallest deployable unit that can be created and managed. It represents a single instance of a running process in your cluster and can contain one or more containers that share storage, network, and a specification on how to run the containers. Pods are ephemeral by nature and can be replaced by Kubernetes in case of node failure or other events.</p> <p>Ingress: Ingress refers to the act of entering or the ability to enter. n the context of networking and computing, it typically denotes incoming traffic to a network or service from an external source.</p> <p>Load balancer: A load balancer is a system that distributes network or application traffic across a number of servers to ensure no single server becomes overwhelmed, improving the reliability and performance of applications. It helps to prevent server overload, manage failover, and increase the availability of a website or service by automatically routing client requests to the most suitable server.</p> <p>High availability (HA): High availability (HA) refers to systems that are designed to be operational and accessible without significant downtime. This is achieved through redundancy and failover mechanisms, ensuring that if one component fails, another can take over seamlessly to maintain service continuity. The goal of HA is to minimize the chances of service interruption due to hardware failures, maintenance, or unexpected outages.</p> <p>Azure:: Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services through Microsoft-managed data centers. It provides a range of cloud services, including those for computing, analytics, storage, and networking. Users can pick and choose from these services to develop and scale new applications, or run existing applications in the public cloud.</p> <p>In Canada, Azure has two regions: Canada Central (CA) and Canada East (CE). Canada Central is located in Toronto and is designed to offer low latency to financial services and other businesses in the area. Canada East, located in Quebec City, provides French language service support and disaster recovery for businesses that require data residency within the province of Quebec.</p> <p>Virtual network (VNet): A virtual network in the cloud is a simulated network environment that provides a logical separation of resources within a cloud computing platform. It enables users to define their own network topology, manage IP addresses, configure firewalls, and set up subnets and route tables, all within a secure and isolated space that mimics the functionality of a traditional network.</p>"},{"location":"generic-architecture/#diagrams","title":"Diagrams","text":"<p>Describe the operation of our implementation for high availability (HA) as well as the redundancy of services within Azure using Kubernetes</p> <pre><code>flowchart\n    subgraph Azure[\"Azure\"]\n        direction TB\n        subgraph CC[\"Canada central (CA)\"]\n            subgraph VNet1[\"VNet-CC\"]\n                subgraph Kubernetes1[\"Kubernetes-CC\"]\n                    direction TB\n                    Ingress1[\"Ingress\"]\n                    Pod1[\"Pod\"]\n                    Pod2[\"Pod\"]\n                    Pod3[\"Pod\"]\n                    Pod4[\"Pod\"]\n                    Pod5[\"Pod\"]\n                    Pod6[\"Pod\"]\n                end\n            end\n        end\n\n        subgraph CE[\"Canada east (CE)\"]\n            subgraph VNet2[\"VNet-CE\"]\n                subgraph Kubernetes2[\"Kubernetes-CE\"]\n                    direction TB\n                    Ingress2[\"Ingress\"]\n                    Pod7[\"Pod\"]\n                    Pod8[\"Pod\"]\n                    Pod9[\"Pod\"]\n                    Pod10[\"Pod\"]\n                    Pod11[\"Pod\"]\n                    Pod12[\"Pod\"]\n                end\n            end\n        end\n\n        VNet1[\"VNet-CC\"] &lt;---&gt;|HA| VNet2[\"VNet-CE\"]\n        Kubernetes1[\"Kubernetes-CC\"] &lt;---&gt;|1:1 redundancy| Kubernetes2[\"Kubernetes-CE\"]\n    end</code></pre> <p>Represents the process of sending a request to our infrastructure within Azure</p> <pre><code>flowchart\n    Client[\"Client\"] --&gt;|request| LB\n    subgraph Azure[\"Azure\"]\n        LB[\"Load balancer\"]\n        LB --&gt; VNet1\n        LB --&gt; VNet2\n        direction LR\n        subgraph CC[\"Canada central (CC)\"]\n            subgraph VNet1[\"VNet-CC\"]\n                subgraph Kubernetes1[\"Kubernetes-CC\"]\n                    direction TB\n                    Ingress1[\"Ingress\"]\n                    Pod1[\"Pod\"]\n                    Pod2[\"Pod\"]\n                    Pod3[\"Pod\"]\n                    Pod4[\"Pod\"]\n                    Pod5[\"Pod\"]\n                    Pod6[\"Pod\"]\n                end\n            end\n        end\n\n        subgraph CE[\"Canada east (CE)\"]\n            subgraph VNet2[\"VNet-CE\"]\n                subgraph Kubernetes2[\"Kubernetes-CE\"]\n                    direction TB\n                    Ingress2[\"Ingress\"]\n                    Pod7[\"Pod\"]\n                    Pod8[\"Pod\"]\n                    Pod9[\"Pod\"]\n                    Pod10[\"Pod\"]\n                    Pod11[\"Pod\"]\n                    Pod12[\"Pod\"]\n                end\n            end\n        end\n    end</code></pre> <p>Representation of the two diagrams above in one</p> <p></p>"},{"location":"generic-architecture/#references","title":"References","text":"<p>Kubernetes</p> <p>Pod</p> <p>HA</p> <p>Load balancer</p>"},{"location":"gh_docker_workflow/","title":"Github workflow to build and push images to GCR","text":""},{"location":"gh_docker_workflow/#executive-summary","title":"Executive summary","text":"<p>The provided text outlines a workflow designed to build and push an image to the GitHub Container Registry. It involves creating three tags: pull request number, pull request name, and commit SHA.</p>"},{"location":"gh_docker_workflow/#glossary","title":"Glossary","text":"<ul> <li> <p>Image: The term \"image\" typically refers to a packaged, standalone software component that includes everything needed to run an application, such as the code, runtime, libraries, and dependencies.</p> </li> <li> <p>GitHub Container Registry (GCR): GCR is a service provided by GitHub that allows users to store, manage, and distribute Docker container images within the GitHub ecosystem. It serves as a centralized repository for container images associated with GitHub repositories.</p> </li> <li> <p>GitHub Action: A GitHub Action is essentially a workflow or automated process defined within a GitHub repository. It enables users to automate tasks such as building, testing, and deploying software directly within the GitHub platform, providing a powerful way to streamline development workflows.</p> </li> </ul>"},{"location":"gh_docker_workflow/#explanation-of-the-diagram","title":"Explanation of the diagram","text":"<p>The diagram illustrates a workflow for building and pushing an image to the GitHub Container Registry. It outlines the steps involved in this process, including the creation of three specific tags: pull request number, pull request name, and commit SHA based on the commit made in a pull request. Once the image is in the GCR, a Kubernetes deployment can use the image</p>"},{"location":"gh_docker_workflow/#diagram","title":"Diagram","text":""},{"location":"gh_docker_workflow/#references","title":"References","text":"<p>Docker</p> <p>Github action</p> <p>Github container registry</p> <p>Kubernetes image</p>"},{"location":"multi-layered-application/","title":"Multi layered application","text":""},{"location":"multi-layered-application/#executive-summary","title":"Executive summary","text":"<p>In our multi layered architecture, the frontend and backend of our applications are intricately linked, with the backend coded in Python and the frontend in TypeScript, each residing in their respective directories. The backend not only processes requests but also occasionally interacts with various object storage solutions, such as AI models, databases, and blob storage, to manage and retrieve data. This interaction is crucial for the seamless operation of our services and is depicted in the accompanying of sequence diagrams, which illustrates the flow of a request from the frontend through the ingress to the backend.</p>"},{"location":"multi-layered-application/#glossary","title":"Glossary","text":"<p>Frontend: Frontend refers to the part of a website or application that users interact with directly, encompassing the design, layout, and behavior that people experience within a web browser or app interface.</p> <p>Backend: The backend refers to the server-side of a web application, encompassing the database, server, and application logic that process user requests and perform the core functional operations of the system.</p> <p>Database: A database is a structured collection of data that is stored and accessed electronically, designed to manage, query, and retrieve information efficiently.</p> <p>Ingress: Ingress refers to the act of entering or the ability to enter. n the context of networking and computing, it typically denotes incoming traffic to a network or service from an external source.</p> <p>Browser: A browser, also known as a web browser, is a software application used to access, retrieve, and view content on the World Wide Web, including webpages,  images, videos, and other multimedia. It interprets HTML and other web technologies to present information in an accessible format.</p>"},{"location":"multi-layered-application/#diagram","title":"Diagram","text":"<p>This diagram shows the communication between the <code>frontend</code>, the <code>backend (/api)</code>, the <code>browser (client)</code>, and the <code>ingress (ingress nginx)</code> for an application.</p> <pre><code>sequenceDiagram\n    participant Browser\n    participant Ingress\n    participant Frontend\n    participant Backend\n\n    Note over Browser,Backend: DNS https://inspection.alpha.canada.ca resolves to Ingress IP with A record\n    Note over Browser,Backend: https://*.inspection.alpha.canada.ca * is any CNAME to the DNS\n\n    Browser-&gt;&gt;Ingress: GET / https://*.inspection.alpha.canada.ca\n    Ingress-&gt;&gt;Frontend: GET /\n    Frontend--&gt;&gt;Ingress: 200\n    Ingress--&gt;&gt;Browser: The browser display the result\n\n    Browser-&gt;&gt;Ingress: GET /api/search https://*.inspection.alpha.canada.ca/api/search/\n    Ingress-&gt;&gt;Backend: GET /search\n    Note over Ingress: /api is /search (ImplementationSpecific)\n    Backend--&gt;&gt;Ingress: 200\n    Ingress--&gt;&gt;Browser: The browser display the result</code></pre>"},{"location":"multi-layered-application/#references","title":"References","text":"<p>Ingress NGINX</p> <p>Ingress NGINX - ImplementationSpecific</p> <p>DNS</p> <p>DNS - A record</p> <p>DNS - CNAME record</p>"},{"location":"networking/","title":"Networking","text":""},{"location":"networking/#executive-summary","title":"Executive summary","text":"<p>Here's how a request is routed to our Kubernetes cluster. Once the image responds with the content, the process is reversed to display the results to the user.</p>"},{"location":"networking/#glossary","title":"Glossary","text":"<p>DNS: Translates domain names (like google.com) into IP addresses (like 172.217.14.238).</p> <p>Ingress NGINX: A controller that uses NGINX as a web server to manage incoming traffic to a Kubernetes cluster. It routes traffic to different services based on URL, hostname, or other criteria.</p> <p>Kubernetes: An open-source container orchestration platform for automating the deployment, scaling, and management of containerized applications.</p> <p>Cert-manager: A tool for managing TLS certificates for Kubernetes. It automates the process of obtaining, renewing, and validating certificates for services exposed on the internet.</p>"},{"location":"networking/#diagrams","title":"Diagrams","text":""},{"location":"networking/#inspectionalphacanadaca","title":".inspection.alpha.canada.ca","text":"<p>As part of the deployments we carry out within the AI Lab, we needed a DNS that would allow us to deploy our services. Since we are not in production, we needed a name that is suitable for an alpha/staging environment that complies with the following guidelines:</p> <ul> <li>Alpha canada.ca</li> <li>Government of Canada Digital Standards</li> </ul> <p>For more information, please refer to this pull request (PR) submitted to cds-snc so that they can subdelegate <code>inspection.alpha.canada.ca</code> to us:</p> <ul> <li>The pull request made to cds-snc (status: merged)</li> </ul>"},{"location":"networking/#references","title":"References","text":"<ul> <li>Cert manager</li> <li>Ingress NGINX</li> <li>DNS</li> <li>Kubernetes</li> </ul>"},{"location":"observabilty/","title":"Implementation documentation for observability Solution","text":""},{"location":"observabilty/#overview","title":"Overview","text":"<p>This documentation explains the implementation of our observability stack using OpenTelemetry, Grafana Alloy, Loki, Tempo, and Prometheus. This stack provides comprehensive monitoring and observability for our applications hosted on Azure Kubernetes Service (AKS).</p>"},{"location":"observabilty/#how-grafana-alloy-works","title":"How Grafana Alloy works","text":"<p>Grafana Alloy acts as the primary OpenTelemetry collector, aggregating metrics, logs, and traces from various sources. It processes the collected data and forwards it to Loki, Tempo, and Prometheus for storage and querying.</p>"},{"location":"observabilty/#configuration-of-grafana-alloy","title":"Configuration of Grafana Alloy","text":"<p>The detailed configuration of Grafana Alloy is available from: alloy-values.yaml.</p>"},{"location":"observabilty/#explanation-of-the-configuration","title":"Explanation of the configuration","text":"<ul> <li>otelcol.exporter.otlp: Configures the OTLP exporter to send data to   specified endpoints.</li> <li>otelcol.receiver.otlp: Sets up OTLP receivers for gRPC and HTTP protocols,   defining how metrics, logs, and traces are received and forwarded.</li> <li>otelcol.processor.memory_limiter: Implements a memory limiter to control   memory usage, ensuring stability.</li> <li>otelcol.processor.batch: Batches telemetry data before forwarding to   improve performance and efficiency.</li> <li>logging: Configures logging level and format for troubleshooting and   monitoring Alloy\u2019s operation.</li> <li>otelcol.exporter.loki: Configures the exporter to send logs to Loki.</li> <li>loki.write: Defines the endpoint for sending logs to Loki.</li> <li>otelcol.exporter.otlp: Configures the exporter to send traces to Tempo.</li> <li>otelcol.exporter.prometheus: Configures the exporter to send metrics to   Prometheus.</li> <li>prometheus.remote_write: Defines the endpoint for sending metrics to   Prometheus.</li> </ul>"},{"location":"observabilty/#exposing-alloy-services","title":"Exposing Alloy services","text":"<p>The following service ports expose Alloy, making it accessible within the Kubernetes cluster:</p> <pre><code>- name: \"grpc\"\n  port: 4317\n  targetPort: 4317\n  protocol: \"TCP\"\n- name: \"http\"\n  port: 4318\n  targetPort: 4318\n  protocol: \"TCP\"\n</code></pre> <p>Accessible via: alloy.monitoring.svc.cluster.local</p>"},{"location":"observabilty/#instrumentation-of-metrics-logs-and-traces","title":"Instrumentation of Metrics, Logs, and Traces","text":"<p>To send metrics, logs, and traces to Grafana Alloy, configure your applications to use the OpenTelemetry SDKs and set the endpoint to alloy.monitoring.svc.cluster.local.</p> <p>Example for Metrics</p> <pre><code>from opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OtlpExporter\n\nmetrics.set_meter_provider(MeterProvider())\nmeter = metrics.get_meter(__name__)\nexporter = OtlpExporter(endpoint=\"alloy.monitoring.svc.cluster.local:4317\")\n</code></pre> <p>Example for Logs</p> <pre><code>from opentelemetry import logs\nfrom opentelemetry.sdk.logs import LogEmitterProvider\nfrom opentelemetry.exporter.otlp.proto.grpc.log_exporter import OtlpExporter\n\nlogs.set_log_emitter_provider(LogEmitterProvider())\nemitter = logs.get_log_emitter(__name__)\nexporter = OtlpExporter(endpoint=\"alloy.monitoring.svc.cluster.local:4317\")\n</code></pre> <p>Example for Traces</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OtlpExporter\n\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\nexporter = OtlpExporter(endpoint=\"alloy.monitoring.svc.cluster.local:4317\")\n</code></pre>"},{"location":"observabilty/#forwarding-data-to-loki-tempo-and-prometheus","title":"Forwarding Data to Loki, Tempo, and Prometheus","text":"<p>Grafana Alloy processes and forwards the received data to the respective backends.</p> <ul> <li>Logs are forwarded to Loki at   http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/push</li> <li>Traces are forwarded to Tempo at   http://tempo.monitoring.svc.cluster.local:4317</li> <li>Metrics are forwarded to Prometheus at   http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/api/v1/write</li> </ul>"},{"location":"observabilty/#using-grafana-for-visualization","title":"Using Grafana for Visualization","text":"<p>Grafana is available at https://grafana.inspection.alpha.canada.ca. Access is granted through the agency's GitHub organization.</p>"},{"location":"observabilty/#setting-up-grafana","title":"Setting Up Grafana","text":"<ol> <li>Login to Grafana using your GitHub credentials.</li> <li>Navigate to Dashboards to explore different directories like admin,    devsecops, finesse, and nachet.</li> <li>Use one of the three data sources:<ul> <li>Prometheus for metrics</li> <li>Loki for logs</li> <li>Tempo for traces</li> </ul> </li> </ol>"},{"location":"observabilty/#creating-dashboards","title":"Creating Dashboards","text":"<ol> <li>Go to Dashboards &gt; New Dashboard.</li> <li>Select the data source (Prometheus, Loki, or Tempo).</li> <li>Create visualizations and configure panels as needed.</li> </ol>"},{"location":"observabilty/#current-dashboards","title":"Current Dashboards","text":"<p>Most dashboards are under the devsecops folder, including:</p> <ul> <li>Node Health Monitoring</li> <li>ArgoCD Activity</li> <li>Ingress-NGINX Traffic</li> <li>Vulnerability Scans by Trivy and Falco</li> </ul>"},{"location":"observabilty/#future-updates","title":"Future Updates","text":"<p>This documentation will be updated as we incorporate dashboards for client applications such as Fertiscan, Finesse and Nachet.</p>"},{"location":"observabilty/#references","title":"References","text":"<ul> <li>OpenTelemetry documentation</li> <li>Python OpenTelemetry SDK</li> <li>Grafana documentation</li> <li>Prometheus documentation</li> <li>Loki documentation</li> <li>Tempo documentation</li> <li>Grafana Alloy documentation</li> </ul>"},{"location":"secrets-management/","title":"Secret management","text":""},{"location":"secrets-management/#introduction","title":"Introduction","text":"<p>Secrets are sensitive pieces of information that should be protected from unauthorized access. In the context of a Kubernetes cluster, secrets are used to store sensitive data such as passwords, tokens, and keys. To allow for secure and efficient management of secrets, we are using HashiCorp Vault, a tool that is designed to manage secrets and protect sensitive data. Vault provides a centralized way to manage access to secrets and encryption keys, and it also has the ability to generate dynamic secrets on demand. This document provides an overview of the secret management process and the role of Vault in securing and managing secrets in the Kubernetes cluster.</p>"},{"location":"secrets-management/#vault-architecture","title":"Vault architecture","text":"<p>Vault is a highly available and distributed system that is designed to provide secure storage and management of secrets. It is built on a client-server architecture, with the server being the central component that stores and manages secrets, and the clients being the applications and services that access the secrets. The server is responsible for authenticating clients, authorizing access to secrets, and providing encryption and decryption services. The server is also responsible for generating dynamic secrets on demand, which are short-lived and are automatically revoked after a certain period of time.</p> <p>Current configuration allows vault to inject secrets into pods secrets using the ArgoCD Vault plugin. The plugin reads placeholders in the YAML files and replaces them with the actual secret values from Vault. This provides a secure way to manage secrets in the Kubernetes cluster and ensures that sensitive data is protected from unauthorized access.</p> <p>The following diagram illustrates the structure of the Vault architecture within howard : </p> <p>The following sequence diagram describes the process of how a developer can update secrets using the Vault UI service and how the secrets are injected into pods :</p> <pre><code>sequenceDiagram\n    participant Developer\n    participant FinesseRepo as Finesse Repository\n    participant GHWorkflow as GitHub Workflow\n    participant ContainerReg as GitHub Container Registry\n    participant HowardRepo as Howard Repository\n    participant ArgoRepoServer as ArgoCD Repo Server\n    participant ArgoVaultPlugin as Argo Vault Plugin\n    participant FinessePod as Finesse Pod\n\n    participant VaultUI as Vault UI\n    participant Vault as Vault Server\n\n    Developer-&gt;&gt;+FinesseRepo: 1. Pushes commits\n    FinesseRepo-&gt;&gt;+GHWorkflow: Triggers workflow\n    GHWorkflow-&gt;&gt;+ContainerReg: Builds and pushes new semantic version\n    GHWorkflow-&gt;&gt;+ArgoRepoServer: Triggers webhook\n    ArgoRepoServer-&gt;&gt;+FinessePod: Triggers synchronisation to pod\n    FinessePod-&gt;&gt;+ContainerReg: Fetches image with new version tag\n    FinessePod-&gt;&gt;+FinessePod: Refreshes deployment with new version\n    Developer-&gt;&gt;+VaultUI: 2. Accesses UI to update/create secrets\n    VaultUI-&gt;&gt;+Vault: Commit update/creation of secrets\n    Developer-&gt;&gt;+HowardRepo: 3. Commits new/updated secrets\n    HowardRepo-&gt;&gt;+ArgoRepoServer: Triggers sync via webhook\n    ArgoRepoServer-&gt;&gt;+ArgoVaultPlugin: triggers refresh on finesse namespace,&lt;br&gt; sync secrets from Vault\n    ArgoVaultPlugin-&gt;&gt;+Vault: Fetch specific version of secrets\n    ArgoVaultPlugin-&gt;&gt;+FinessePod: Injects secrets\n    Developer-&gt;&gt;+FinessePod: 4. Trigger hard refresh through argoCD</code></pre> <p>Take note that the developer needs to trigger a hard refresh on the pod to reflect the changes in the secrets. This is done in the ArgoCD UI, but we are working on a way to automate this process.</p>"},{"location":"secrets-management/#secret-management-process","title":"Secret management process","text":"<p>The secret management process involves the following steps:</p> <ol> <li> <p>Secret creation: Secrets are created and stored in Vault using the Vault    CLI or API. When a secret is created, it is encrypted and stored in the    central Vault server.</p> </li> <li> <p>Secret retrieval: Applications and services can retrieve secrets from     Vault using the Vault CLI or API. When a secret is retrieved, it is     decrypted and returned to the client in a secure manner.</p> </li> <li> <p>Dynamic secret generation: Vault has the ability to generate dynamic     secrets on demand. This means that instead of storing static secrets in     Vault, Vault can generate short-lived secrets that are automatically revoked     after a certain period of time. This provides an additional layer of     security and reduces the risk of unauthorized access to secrets.</p> </li> <li> <p>Access control: Vault provides fine-grained access control to secrets,     allowing administrators to define policies that specify which clients can     access which secrets. This ensures that only authorized clients can access     sensitive data. Currently, we are using the Kubernetes authentication method     to authenticate hosted applications and authorize access to secrets. As for     the human users, we are using the Github authentication method to     authenticate and authorize access to secrets.</p> </li> </ol>"},{"location":"secrets-management/#create-read-update-and-delete-secrets","title":"Create, read, update, and delete secrets","text":"<p>Vault provides a UI service to manage secrets. The UI service is a web-based user interface that allows administrators to create, read, update, and delete secrets. The service also provides a way to manage access control policies and audit logs. The service is accessible through a web browser and is protected by the same security mechanisms as the Vault server.</p>"},{"location":"secrets-management/#steps-to-update-secrets-values-using-the-vault-ui","title":"Steps to update secrets values using the Vault UI","text":"<ol> <li>In order to gain access to the Vault UI service, you need to have the    appropriate permissions and access to the Vault URL. It is currently    configured to give access to any member of the <code>ai-cfia</code> organization on    Github.</li> <li>Generate a personal access token on Github and use it to authenticate to the    Vault UI service. The scope of the token should be : </li> <li>Gain access to the Vault UI service by navigating to the Vault URL in a web    browser. You will be prompted to authenticate using your Github PAT token.</li> <li>Once authenticated, you will be able to create, read, update, and delete    secrets using the UI service. Simply navigate to the PV secret engine and    follow the path to your applications secrets. The PV secret engine is a     key-value store that allows you to store and manage secrets for your     applications. </li> <li>Once in the directory of your application secrets, simply click on 'create     new version' and you will be able to add, update, or delete secrets as     needed. </li> </ol>"},{"location":"secrets-management/#steps-to-update-secrets-injected-into-pods","title":"Steps to update secrets injected into pods","text":"<p>In order to update secrets that are injected into pods, you need to update the secret manifest for the application. The secret manifest is a YAML file that defines the secrets that are injected into the pod's as environment variables. We will use Finesse as an example.</p> <ol> <li>Open an issue with the following template : Secrets update template. You can then create a working    branch from the issue.</li> <li>Open <code>/kubernetes/aks/apps/finesse/base/finesse-secrets.yaml</code>.</li> <li>Update the secrets key references as needed. For example, to add a new    secret, you can add a new key-value pair to the <code>data</code> section of the secret   manifest :</li> </ol> <pre><code>FINESSE_BACKEND_AZURE_SEARCH_TRANSFORM_MAP: &lt;FINESSE_BACKEND_AZURE_SEARCH_TRANSFORM_MAP&gt;\n</code></pre> <p>The key represents the environment variable name that will be injected into the    pod, and the value represents the secret key in Vault that will be used to fetch    the secret value.</p> <ol> <li>Update the version annotation of  the secrets being fetch from vault :</li> </ol> <pre><code># Bump the version of the secret from\navp.kubernetes.io/secret-version: \"4\"\n# To\navp.kubernetes.io/secret-version: \"5\"\n</code></pre> <p>This is the new version that we create in step 5 of the previous section.</p> <p>As additional example, here is an issue and a pull request that showcases the    process of updating secrets in the Nachet application :</p> <ul> <li>Issue</li> <li>Pull request</li> </ul>"},{"location":"secrets-management/#argo-cd-vault-plugin-avp","title":"Argo CD Vault plugin (AVP)","text":"<p>The argocd-vault-plugin is used to manage secrets inside our deployments the Gitops way. It allows to use <code>&lt;placeholders&gt;</code> in any YAML or JSON files that have been templated and make use of annotations to provide the path and version of a secret inside vault.</p> <p>An example of usage is showcased inside the demo app sample. The official documentation for the plugin is well explained and can be followed according to the usecase needed.</p>"},{"location":"terraform-workflow/","title":"Terraform workflow for managing resources in Azure","text":"<p>This section serves as a guide for utilizing the Azure DevOps pipeline configured in the 'AI-Lab' repository, which is designed for managing infrastructure deployments through Terraform within our organization. The pipeline code currently resides on Github since thats where the infrastructure code is. It is currently configured with Azure Devops since its the only current way of having access to a service account allowing us to create resources on our Azure subscription.</p> <p>The following diagram illustrates the Terraform workflow for managing resources : </p>"},{"location":"terraform-workflow/#getting-started","title":"Getting Started","text":"<ul> <li>Prerequisites: Ensure you have necessary access to the Azure DevOps  project and the 'AI-Lab' repository.</li> <li>Repository Setup: The pipeline apply-terraform.yml is linked to Azure  DevOps through a service connector.</li> </ul>"},{"location":"terraform-workflow/#approval-process-flow","title":"Approval Process Flow","text":"<p>The pipeline includes an essential approval step before applying any Terraform plan to ensure that all changes are reviewed and authorized. This step is crucial for maintaining control over the infrastructure changes and ensuring they meet our operational and security standards.</p>"},{"location":"terraform-workflow/#production-approval-environment","title":"Production Approval Environment","text":"<ul> <li>The <code>ProductionApproval</code> environment in Azure DevOps is configured to require  manual approval before the Terraform apply step can proceed.</li> <li>This environment is linked to our Azure subscription through a service  connection, which has the necessary permissions to apply a terraform plan.</li> </ul>"},{"location":"terraform-workflow/#how-to-approve-changes","title":"How to Approve Changes","text":"<ol> <li>Review Plan: When a Terraform plan is triggered by a commit to the <code>main</code>   branch, it will first initialize and plan the infrastructure changes without   applying them.</li> <li>Approval Notification: Relevant stakeholders will receive a notification   (via email or within Azure DevOps) requesting approval for the planned   changes.</li> <li>Accessing the Approval Request: Navigate to the <code>Environments</code> section   within the Azure DevOps project, and select the <code>ProductionApproval</code>   environment to view pending approvals.</li> <li>Review and Approve: Review the details of the planned changes. If they   align with our standards and expectations, approve the deployment. If not, you   may reject or discuss the changes further with the team.</li> </ol>"},{"location":"adr/010-infrastructure.en-ca/","title":"ADR-010: Infrastructure","text":""},{"location":"adr/010-infrastructure.en-ca/#executive-summary","title":"Executive Summary","text":"<p>In an effort to optimize and secure our infrastructure operations, our organization has adopted a strategy based on Infrastructure as Code (IaC) using Terraform, complemented by the deployment of a Kubernetes cluster on Azure. This approach allows us to overcome the limitations associated with traditional methods such as ClickOps and manual deployments, which were both time-consuming and error-prone. The adoption of HashiCorp Vault for centralized secret management and ArgoCD for deployment orchestration strengthens our security and agility posture. By integrating advanced monitoring solutions and considering the use of technologies like OpenTelemetry for enhanced observability, we aim to maintain high availability and performance of our services. This transformation allows for more robust and automated infrastructure management, reduces the risks of human error, and provides increased flexibility and portability across different cloud environments. Our initiative aligns infrastructure management with our operational goals while ensuring enhanced scalability and security to meet future needs.</p>"},{"location":"adr/010-infrastructure.en-ca/#context","title":"Context","text":"<p>Our team faces challenges in deploying solutions, especially in choosing cloud providers. Initially, we used Google Cloud Run and Azure App Service. However, due to the absence of a Google Cloud account and access restrictions on Azure, we find ourselves switching from one account to another, resulting in significant downtime for our applications.</p> <p>Moreover, the manual creation of all services on cloud providers via ClickOps proved tedious. To overcome this challenge, we decided to adopt Infrastructure as Code (IaC) using Terraform. This approach allows us to manage and provision our cloud infrastructures via codified configuration files, thus eliminating the need for ClickOps and significantly reducing human errors.</p> <p>Regarding security, we initially adopted Azure Key Vault for the manual retrieval of environment variable values. However, recognizing the need for a more robust and versatile solution for secret management, we have evolved towards maintaining a HashiCorp Vault instance. This transition enables centralized management of secrets and credentials across different environments and platforms.</p> <p>Currently, scaling our applications is not a priority, as we have a fixed visibility on the number of users. However, we have not yet implemented a scaling solution.</p> <p>For monitoring and telemetry, we currently rely exclusively on the built-in tools of cloud providers, such as those from Google Cloud Run. However, it is important to consider the flexibility and portability that external services such as OpenTelemetry can offer. These solutions can not only adapt to various cloud environments but also provide custom customization specifically tailored to our needs. Although in-house solutions may seem demanding in terms of maintenance, they allow us to optimize our monitoring and telemetry in a targeted way, thus offering a more precise alignment with our operational goals.</p> <p>In short, many tasks are currently performed manually. Although we have a Github Workflow for deploying Docker images, the management of deployments across different cloud providers is not automated. In the event of a production error, no solution allows developers to quickly resolve the issue.</p>"},{"location":"adr/010-infrastructure.en-ca/#use-cases","title":"Use Cases","text":"<ul> <li>Manage PostgreSQL database (and soon PostgreSQL ML) without resorting to   ClickOps.</li> <li>Increase data redundancy more effectively.</li> <li>Deploy, manage, monitor, and instrument applications within the organization.</li> <li>Improve secret management.</li> <li>Eliminate silos between the security team and the DevOps team within the   organization.</li> <li>Implement deployments across all cloud providers in case of outages. This   includes data persistence across different cloud providers.</li> <li>Manage a centralized SSO solution to authenticate users of hosted services.</li> <li>Use Infrastructure as Code to automate the creation, deployment, and   management of infrastructure, enabling faster infrastructure operations while   reducing manual errors.</li> <li>Automate scaling (HPA).</li> <li>Adopt a backup and disaster recovery strategy.</li> <li>Create documentation that is easy to read and adapt to enable a \"shift-left\"   transition (Early and thorough integration of testing, security, and quality   assurance at the beginning of the software development cycle, for earlier   identification and resolution of anomalies).</li> <li>Avoid single points of failure.</li> </ul>"},{"location":"adr/010-infrastructure.en-ca/#decision","title":"Decision","text":"<p>Our solution will consist of deploying Kubernetes clusters on various cloud providers. Here are the components that will be deployed to manage various use cases:</p> <ul> <li>Container management and deployment: Kubernetes</li> <li>Secret management: HashiCorp Vault</li> <li>Deployment management: ArgoCD</li> <li>Infrastructure as Code (IaC) management: Terraform</li> <li>Development environment management: AzureML (coming soon)</li> <li>User authentication management:   Vouch-proxy</li> <li>Observability management: Grafana, Prometheus, Open-Telemetry, and OneUptime   (coming soon)</li> <li>Load balancing management: Ingress NGINX</li> <li>Security management: Trivy and Falco</li> <li>Managing redundancy: Itsio / Cluster mesh (coming soon)</li> </ul> <p>Additional components will be added as needed.</p>"},{"location":"adr/010-infrastructure.en-ca/#consequences","title":"Consequences","text":"<p>The transition to Kubernetes-based infrastructure management and Terraform, combined with the use of robust solutions for secret management (HashiCorp Vault) and deployment (ArgoCD), marks significant progress towards full automation and increased security of our cloud environment.</p> <p>This approach minimizes manual interventions and error risks while enhancing security at every stage of application deployment. Using open-source tools promotes greater transparency, adaptability to multiple environments, and easier integration with various ecosystems. Furthermore, adopting GitOps practices, notably through Terraform and ArgoCD, improves the traceability and reversibility of changes made to the infrastructure, essential for configuration management and security compliance. These changes support our ability to scale quickly and reliably while maintaining strict control over data security and user authentication through Vouch-proxy and integrating solutions such as NGINX Ingress for access management. However, this evolution requires ongoing skill development of our teams and sustained attention to updates and maintenance of these technologies to ensure their effectiveness and security over the long term.</p>"},{"location":"adr/010-infrastructure.en-ca/#references","title":"References","text":"<ul> <li>Howard Repository - Contains the configuration of our infrastructure along   with documentation</li> </ul>"},{"location":"adr/011-gitops.en-ca/","title":"ADR-011: GitOps","text":""},{"location":"adr/011-gitops.en-ca/#introduction","title":"Introduction","text":"<p>This document outlines the decision to use ArgoCD as the continuous deployment tool for our Kubernetes applications.</p>"},{"location":"adr/011-gitops.en-ca/#background","title":"Background","text":"<p>Before implementing ArgoCD, the process of addressing production issues was manual and time-consuming. A developer had to report an issue to a DevSecOps, which could result in a waiting period before the issue was resolved.</p>"},{"location":"adr/011-gitops.en-ca/#use-cases","title":"Use Cases","text":"<ul> <li> <p>Developers can deploy and test their changes without waiting for a DevSecOps intervention.</p> </li> <li> <p>Developers can identify and resolve production issues more quickly.</p> </li> <li> <p>Development and operations teams can work more closely together.</p> </li> </ul>"},{"location":"adr/011-gitops.en-ca/#decision","title":"Decision","text":"<p>The team has already had positive experiences with ArgoCD.</p>"},{"location":"adr/011-gitops.en-ca/#considered-alternatives","title":"Considered Alternatives","text":""},{"location":"adr/011-gitops.en-ca/#flux","title":"Flux","text":"<p>Advantages:</p> <ul> <li>Easy to set up</li> </ul> <p>Disadvantages:</p> <ul> <li>No user interface</li> </ul>"},{"location":"adr/011-gitops.en-ca/#consequences","title":"Consequences","text":"<ul> <li> <p>Developers will be able to deploy and test their changes more quickly.</p> </li> <li> <p>Production issues can be resolved more swiftly.</p> </li> <li> <p>Development and operations teams can work more closely together.</p> </li> </ul>"},{"location":"adr/011-gitops.en-ca/#references","title":"References","text":"<ul> <li>ArgoCD ACIA/CFIA url</li> <li>Document on Secret Management</li> </ul>"},{"location":"adr/012-secret-management.en-ca/","title":"ADR-012: Secret Management","text":""},{"location":"adr/012-secret-management.en-ca/#executive-summary","title":"Executive Summary","text":"<p>After thorough analysis of the available options for secret management, we have decided to adopt HashiCorp Vault. This solution stands out for its strong integration with Kubernetes, its ability to manage secrets within applications, its integration with ArgoCD, and its secure method of secret sharing. These features align Vault with our objectives of agility, security, and efficiency in managing secrets for our hosted services.</p>"},{"location":"adr/012-secret-management.en-ca/#context","title":"Context","text":"<p>Secure management of secrets is crucial for the protection of sensitive information and securing our infrastructure. In our environment, where Kubernetes plays a central role in managing hosted services, it is essential to choose a secret management solution that integrates well with this ecosystem, while offering flexibility and security. Moreover, our use of ArgoCD for deployment automation requires that our secret management solution integrate seamlessly, facilitating consistent and secure secret management across our CI/CD pipelines.</p>"},{"location":"adr/012-secret-management.en-ca/#decision","title":"Decision","text":"<p>We have chosen HashiCorp Vault as our primary solution for secret management. Vault offers tight integration with Kubernetes, enabling effective secret management within applications. This integration is demonstrated through the Vault Kubernetes Auth Method, which allows applications running in Kubernetes to access secrets stored in Vault using Kubernetes service tokens for authentication.</p> <p>Vault also facilitates dynamic rotation of secrets, ensuring that sensitive information does not remain static and is regularly updated without manual intervention. This feature is crucial for maintaining a high security posture and reducing the attack surface related to compromised secrets.</p> <p>Additionally, Vault can generate on-demand secrets for databases and other services, reducing the need for storing static secrets and increasing the efficiency of secret management. Vault's integration with ArgoCD through plugins such as the Argo CD Vault Plugin simplifies application deployment by ensuring that necessary secrets are securely injected at the time of deployment.</p> <p>Choosing a centralized and self-hosted instance of Vault gives complete control over secret management and ensures compliance with data sovereignty requirements by allowing us to store and manage secrets within our own infrastructure without relying on third parties.</p> <p>Adopting Vault also facilitates the implementation of GitOps practices for configuration and secret management, ensuring that all changes are traceable, auditable, and deployed through continuous integration and deployment processes.</p>"},{"location":"adr/012-secret-management.en-ca/#considered-alternatives","title":"Considered Alternatives","text":""},{"location":"adr/012-secret-management.en-ca/#azure-key-vault","title":"Azure Key Vault","text":"<p>Advantages:</p> <ul> <li>Native integration with the Azure ecosystem, facilitating secret management   for Azure resources.</li> </ul> <p>Disadvantages:</p> <ul> <li> <p>Primarily limited to the Azure ecosystem, presenting challenges for   multi-cloud or hybrid deployments.</p> </li> <li> <p>Less native integration with Kubernetes compared to HashiCorp Vault, which   could complicate secret management in our Kubernetes applications.</p> </li> <li> <p>Azure Key Vault cannot be self-hosted, which may pose data sovereignty and   compliance issues.</p> </li> <li> <p>The GitHub action for Azure Key Vault is obsolete and no longer maintained.   The only way to retrieve secrets from Azure Key Vault in a pipeline is by   using a custom script. This is explained in their archived repository.</p> </li> <li> <p>Azure Key Vault does not support secret versioning and does not allow for   GitOps management of secrets.</p> </li> </ul>"},{"location":"adr/012-secret-management.en-ca/#mozilla-sops","title":"Mozilla SOPS","text":"<p>Advantages:</p> <ul> <li>SOPS is a secret management tool that encrypts/decrypts file content with a   key derived from AWS, KMS, GCP KMS, Azure Key Vault, or PGP, ensuring that   secrets are always encrypted and can be easily integrated into version control   systems.</li> </ul> <p>Disadvantages:</p> <ul> <li>SOPS is not a secret management solution that fulfills all our needs   autonomously. It does not manage the storage of secrets in a centralized   location or the rotation of secrets.</li> </ul>"},{"location":"adr/012-secret-management.en-ca/#consequences","title":"Consequences","text":"<p>The adoption of HashiCorp Vault will enable us to enhance the security of our infrastructure by centralizing secret management in a tool specifically designed for this purpose. Integration with Kubernetes and ArgoCD supports our automation and efficiency efforts, ensuring that secrets are managed and deployed securely in our environment. This decision will require training our teams on Vault and adjusting our existing processes to fully integrate Vault into our CI/CD pipeline.</p>"},{"location":"adr/012-secret-management.en-ca/#references","title":"References","text":"<ul> <li>Documentation on our configuration and use of HashiCorp   Vault</li> <li>Official HashiCorp Vault   Documentation</li> <li>HashiCorp Vault integration with   Kubernetes</li> <li>Guide on HashiCorp Vault integration with   ArgoCD</li> <li>Vault-action for integrating Vault into our CI/CD   pipelines</li> <li>Azure Key Vault   Overview</li> <li>Mozilla SOPS Repository</li> <li>Obsolete Azure key vault   action</li> </ul>"},{"location":"adr/013-IaC-tool.en-ca/","title":"ADR-013: Management of Infrastructure as Code (IaC)","text":""},{"location":"adr/013-IaC-tool.en-ca/#executive-summary","title":"Executive Summary","text":"<p>We have decided to adopt Terraform as our Infrastructure as Code (IaC) solution for managing and deploying our infrastructure across multiple cloud providers. This decision is based on Terraform's cross-cloud compatibility, our extensive knowledge of the tool, and its state tracking capabilities. Terraform also offers broader maturity and community support, without being tied to any specific cloud provider, and allows the creation of custom extensibility providers.</p>"},{"location":"adr/013-IaC-tool.en-ca/#context","title":"Context","text":"<p>As part of our effort to enhance the efficiency and reproducibility of managing our cloud infrastructure, we evaluated several IaC tools. Our infrastructure, while primarily hosted on Azure at present, could expand to other providers in the future, highlighting the need for a flexible and powerful IaC tool.</p> <p>Using an IaC tool allows us to benefit from version control of our infrastructure, similar to what Git provides for source code. This means we can track, review, and reverse infrastructure changes in a controlled and documented manner. Additionally, an IaC tool significantly reduces the time required for provisioning and managing infrastructure, as it allows deploying and updating infrastructure through configuration files instead of manual procedures. This approach also ensures a more consistent environment, reducing discrepancies between development, test, and production environments. Finally, by automating infrastructure management, we minimize the risk of human errors, thus enhancing the security and reliability of our systems.</p>"},{"location":"adr/013-IaC-tool.en-ca/#decision","title":"Decision","text":"<p>After thorough evaluation, we have chosen Terraform as our infrastructure management tool. This decision is based on several key factors: our existing experience with this tool, Terraform's cross-cloud compatibility through a consistent workflow model using a descriptive language, and its state file functionality that facilitates tracking infrastructure changes. Moreover, Terraform's maturity and its vast community provide additional assurance regarding the tool's reliability and scalability. Terraform also allows us to create custom extensibility providers, a crucial feature for our project that requires integration with HashiCorp Vault and GitHub. Thus, Terraform enables us to utilize providers such as Helm, Kubernetes, ArgoCD, Azure DevOps, GitHub, Ansible, and many others.</p>"},{"location":"adr/013-IaC-tool.en-ca/#considered-alternatives","title":"Considered Alternatives","text":""},{"location":"adr/013-IaC-tool.en-ca/#bicep","title":"Bicep","text":"<p>Advantages:</p> <ul> <li>Native integration with Azure, offering a simplified experience for managing   Azure resources.</li> <li>Simplified syntax compared to older tools like Azure Resource Manager (ARM)   templates.</li> </ul> <p>Disadvantages:</p> <ul> <li>No integrated state management: unlike Terraform, which has a state file   feature that allows tracking and managing changes in infrastructure (if an   item is removed from the deployment code, Terraform proceeds with its   destruction), Bicep does not have a similar concept. This means that   modifications or deletions of items require manual intervention or additional   logic to manage the state of the infrastructure.</li> <li>Primarily limited to the Azure ecosystem, which poses challenges for   multi-cloud deployments.</li> <li>Creation of custom extensibility providers is still in the experimental phase.</li> </ul>"},{"location":"adr/013-IaC-tool.en-ca/#cloudformation","title":"CloudFormation","text":"<p>Advantages:</p> <ul> <li>Deep integration with the AWS ecosystem, facilitating the management of AWS   resources.</li> </ul> <p>Disadvantages:</p> <ul> <li>Confined to the AWS ecosystem, limiting flexibility for multi-cloud   deployments.</li> <li>Less support for custom extensibility compared to Terraform.</li> </ul>"},{"location":"adr/013-IaC-tool.en-ca/#pulumi","title":"Pulumi","text":"<p>Advantages:</p> <ul> <li>Familiar Programming Languages: Pulumi allows developers to use   general-purpose programming languages such as TypeScript, Python, Go, and   .NET, which can reduce the learning curve and facilitate integration into   existing CI/CD pipelines.</li> <li>Cloud-Managed State: Pulumi manages the infrastructure state in the cloud,   offering a centralized and secure approach to tracking infrastructure   deployments.</li> <li>Multi-Cloud and On-Premise Support: Pulumi provides broad support for cloud   providers including Azure, AWS, Google Cloud, as well as on-premise resources,   offering great flexibility for hybrid and multi-cloud deployments.</li> </ul> <p>Disadvantages:</p> <ul> <li>Potential complexity: Using full programming languages can introduce   additional complexity into infrastructure management, especially for teams not   familiar with these languages.</li> <li>Dependency on Pulumi Service: While Pulumi offers options to manage state   locally or in the cloud, the cloud-managed version creates a dependency on   Pulumi's services, raising concerns about data sovereignty or additional   costs.</li> <li>Less mature than Terraform: Pulumi is newer to the IaC market than Terraform,   meaning it might have a smaller community and fewer resources available for   troubleshooting and learning compared to Terraform.</li> </ul>"},{"location":"adr/013-IaC-tool.en-ca/#consequences","title":"Consequences","text":"<p>Adopting Terraform as our primary IaC tool will allow us to unify the management of our infrastructure across various providers, thus improving efficiency and reducing the risk of manual errors. This will require ongoing training for our teams to fully leverage Terraform's capabilities. We also anticipate improved collaboration among development, operations, and security teams through a consistent approach to infrastructure management. The decision supports our goal of agility and security in the development and deployment of our applications, while promoting the use of open source solutions for greater transparency and collaboration.</p>"},{"location":"adr/013-IaC-tool.en-ca/#references","title":"References","text":"<ul> <li>Documentation on workflow with Terraform for deploying our services</li> <li>Infrastructure source code   (Howard)</li> <li>Official Terraform website</li> <li>Bicep   documentation</li> <li>Pulumi Github repository</li> <li>AWS CloudFormation user guide</li> <li>Experimental phase for extensibility providers in   Bicep</li> </ul>"},{"location":"adr/014-containers.en-ca/","title":"ADR-014: Containerization","text":""},{"location":"adr/014-containers.en-ca/#executive-summary","title":"Executive Summary","text":"<p>The use of containers and Kubernetes has proven effective for the management and deployment of our applications. This approach provides several benefits, including portability, which allows containers to run consistently across different environments, thereby simplifying development and deployment. Scalability is also a strong point, as Kubernetes enables easy scaling of applications based on demand. In terms of reliability, containers and Kubernetes offer improved fault tolerance and high availability. Lastly, efficiency is enhanced through the use of containers, which optimize resource utilization and reduce costs.</p>"},{"location":"adr/014-containers.en-ca/#background","title":"Background","text":"<p>Previously, we manually deployed each Dockerfile on Google Cloud Run. This approach required significant time investment, as each deployment necessitated manual intervention for building and deploying containers. Despite the systematic presence of Dockerfiles in our repositories, the process of development and deployment remained relatively unautomated.</p>"},{"location":"adr/014-containers.en-ca/#decision","title":"Decision","text":"<p>To overcome these limitations, we decided to adopt Kubernetes for container orchestration. This transition allows us to benefit from more robust and scalable management of our applications. However, deployment is not fully automated; Kubernetes manifests must be created and then deployed via ArgoCD, providing an additional level of control and validation before final deployment.</p> <p>Regarding Docker images, we have set up a GitHub workflow that automates the process of building, tagging, and uploading. This automation helps ensure the consistency and reliability of our deployments while reducing manual workload and error risks.</p>"},{"location":"adr/014-containers.en-ca/#considered-alternatives","title":"Considered Alternatives","text":""},{"location":"adr/014-containers.en-ca/#docker-swarm","title":"Docker Swarm","text":"<p>Advantages:</p> <ul> <li> <p>Native Integration with Docker: Docker Swarm is tightly integrated with Docker and uses the Docker API, which makes setup and management simpler for teams already familiar with Docker.</p> </li> <li> <p>Ease of Deployment: Docker Swarm is simpler to configure and manage compared to Kubernetes, which can be beneficial for small to medium infrastructures without complex needs.</p> </li> <li> <p>Performance: Docker Swarm often shows better performance in terms of container startup time and resource usage.</p> </li> </ul> <p>Disadvantages:</p> <ul> <li> <p>Limited Scalability: Docker Swarm is less suited for large clusters than Kubernetes. Kubernetes excels in managing large, complex clusters.</p> </li> <li> <p>Less Robust Features: Compared to Kubernetes, Docker Swarm offers fewer advanced features, such as sophisticated deployment strategies, advanced volume management, and extensive support for CI/CD tools.</p> </li> <li> <p>Smaller Community: The community around Docker Swarm is less active than that of Kubernetes, which can impact support and the development of new features.</p> </li> </ul>"},{"location":"adr/014-containers.en-ca/#nomad","title":"Nomad","text":"<p>Advantages:</p> <ul> <li> <p>Simplicity and Flexibility: Nomad is known for its simplicity and flexibility. This makes the orchestrator ideal for both non-containerized applications and containers.</p> </li> <li> <p>Heterogeneous Resources and Multi-region: Nomad can manage workloads on different types of servers, including bare metal, VMs, or containerized environments, and can easily handle multi-region clusters.</p> </li> <li> <p>High Performance and Efficiency: Nomad is designed to perform well at large scales, offering quick task start-ups and minimal overhead.</p> </li> </ul> <p>Disadvantages:</p> <ul> <li> <p>Fewer Features than Some Competitors: Although appreciated for its simplicity, Nomad may lack some advanced features found in other orchestrators like Kubernetes.</p> </li> <li> <p>More Limited Ecosystem: The ecosystem around Nomad is less developed compared to more mature solutions like Kubernetes. This can result in a more limited offering in terms of third-party tools, plugins, or integrations, making some tasks more complex to implement and maintain.</p> </li> </ul>"},{"location":"adr/014-containers.en-ca/#conclusion","title":"Conclusion","text":"<p>The adoption of Kubernetes has proven to be a wise decision for the management and deployment of our containerized applications. This approach has improved the portability, scalability, reliability, and efficiency of our applications.</p>"},{"location":"adr/014-containers.en-ca/#references","title":"References","text":"<ul> <li>Kubernetes</li> <li>Docker</li> <li>Docker swarm</li> <li>Nomad</li> </ul>"},{"location":"adr/015-authentication-management.en-ca/","title":"ADR-015: User Authentication Management","text":""},{"location":"adr/015-authentication-management.en-ca/#executive-summary","title":"Executive Summary","text":"<p>As part of the centralized management of user authentication for our applications, particularly the Nachet application, we have chosen to implement Vouch-proxy. This solution was selected for its ability to efficiently integrate authentication services such as Azure and Github, while retrieving user group information in a token. Vouch-proxy allows us to apply uniform and secure authentication management across all services hosted on our Kubernetes cluster, thanks to its integration with NGINX ingress. This choice aims to improve security while offering increased flexibility and scalability for our identity and access management operations.</p>"},{"location":"adr/015-authentication-management.en-ca/#context","title":"Context","text":"<p>Some of our applications, including the Nachet application, require authentication to access their services. Initially, Nachet was deployed without authentication mechanisms, allowing any user to view the page and potentially insert malicious images. Given this vulnerability, and to secure access to all applications on our Kubernetes cluster, it became imperative to adopt a centralized authentication solution. This solution must also be capable of integrating with identity providers such as Azure and Github, and retrieving user groups in a token for fine-grained authorization management.</p>"},{"location":"adr/015-authentication-management.en-ca/#decision","title":"Decision","text":"<p>After evaluating several alternatives, we opted for the use of Vouch-proxy to meet our needs for centralized authentication. Vouch-proxy stands out for its compatibility with identity providers Azure and Github, thus facilitating user authentication while retrieving essential group information in a token. This capability is crucial for managing permissions within our services. Moreover, Vouch-proxy integrates seamlessly with the NGINX ingress of our Kubernetes cluster, enabling centralized management of authentication for all our services. This centralized approach ensures enhanced security and better consistency in managing user access across different applications.</p>"},{"location":"adr/015-authentication-management.en-ca/#considered-alternatives","title":"Considered Alternatives","text":""},{"location":"adr/015-authentication-management.en-ca/#ori-network-ori-oathkeeper-ori-kratos","title":"Ori network (Ori Oathkeeper, Ori Kratos)","text":"<p>Advantages:</p> <ul> <li>Ori Oathkeeper is an authentication and authorization proxy that can be used   to centrally manage the authentication of users and services. Ori Kratos is an   identity and access service that can be used to centrally manage users and   roles. The advantage of these solutions is that they can be used as a complete   solution for managing the authentication and authorization of users and   services.</li> </ul> <p>Disadvantages:</p> <ul> <li>Ori Oathkeeper and Ori Kratos are relatively new solutions and have proven to   be less mature than other authentication solutions. We attempted a deployment   of Kratos and encountered configuration issues, among others. The   documentation is also less comprehensive than that of other solutions.</li> </ul>"},{"location":"adr/015-authentication-management.en-ca/#oauth2-proxy","title":"Oauth2-Proxy","text":"<p>Advantages:</p> <ul> <li>Oauth2-Proxy is an authentication proxy that can be used to manage user   authentication and can be configured with NGINX ingress.</li> </ul> <p>Disadvantages:</p> <ul> <li>Oauth2-Proxy is configurable to manage authentication on a one-for-one basis   but is not designed to handle centralized authentication for multiple   services. Vouch-proxy is a more suitable alternative for our needs for   centralized authentication management.</li> </ul>"},{"location":"adr/015-authentication-management.en-ca/#custom-authentication-solution","title":"Custom Authentication Solution","text":"<p>Advantages:</p> <ul> <li>Precise adaptation: A custom solution offers the possibility to develop an   authentication system that precisely matches the specific requirements of our   infrastructure and applications. This allows for finer integration and   adaptation to internal processes.</li> <li>Complete control: By designing our own solution, we have total control over   security and functionality aspects, allowing us to adjust or improve the   solution based on evolving security needs and user management.</li> </ul> <p>Disadvantages:</p> <ul> <li>High development cost and time: Designing a custom solution requires a   significant initial investment in time and human resources.</li> <li>Maintenance and updating: Post-development operational costs can be   significant, including regular maintenance and updates necessary to meet new   security threats and legal requirements.</li> <li>Competitiveness and maturity: It is challenging for an in-house solution to   compete with the features and security offered by proven solutions on the   market, which benefit from continuous development and feedback from a broad   user base.</li> </ul>"},{"location":"adr/015-authentication-management.en-ca/#consequences","title":"Consequences","text":"<p>The adoption of Vouch-proxy for centralized authentication within our infrastructure has significant implications, both positive and negative. On the positive side, this decision will enhance the security of our applications by effectively integrating robust authentication services such as Azure and Github, while facilitating the management of access rights through the retrieval of user groups in a token. This will also simplify the administration of our systems and improve the consistency of the user experience across different applications. However, it is essential to recognize that this implementation will require technical adjustments, particularly in configuring and maintaining the NGINX ingress, and could introduce initial complexity during the integration phase. Moreover, this decision commits the company to the longevity and evolution of Vouch-proxy, as well as the stability of its integrations with identity providers. It will therefore be crucial to closely monitor these aspects and plan actions for necessary updates and potential adjustments in response to challenges that may arise.</p>"},{"location":"adr/015-authentication-management.en-ca/#references","title":"References","text":"<ul> <li>Oauth2-Proxy</li> <li>Vouch-proxy</li> <li>Ori Oathkeeper</li> <li>Ori Kratos</li> <li>Example of implementing authentication with Express Web   app</li> </ul>"},{"location":"adr/016-networking.en-ca/","title":"ADR-016: Networking","text":""},{"location":"adr/016-networking.en-ca/#executive-summary","title":"Executive Summary","text":"<p>This Architectural Decision Record (ADR) is aimed at formalizing the networking strategy for our applications deployed on Kubernetes, assessing both current and future network access components to maximize performance, security, and ease of management. We use Azure for this architecture and integrate Nginx Ingress, cert-manager, vouch-proxy, and automated DNS record management through external-dns.</p>"},{"location":"adr/016-networking.en-ca/#background","title":"Background","text":"<p>In our current Kubernetes architecture, we utilize Nginx Ingress for access control, managed by a static IP assigned by Azure. We automate the creation of CNAME records for our services via external-dns, which significantly facilitates DNS management. Additionally, cert-manager is deployed for automatic SSL certificate management, thereby securing communications, while vouch-proxy manages secure access to our applications. Our domain <code>.inspection.alpha.canada.ca</code> is managed within a DNS zone on Azure, benefiting from direct integration with these services.</p>"},{"location":"adr/016-networking.en-ca/#decision","title":"Decision","text":"<p>We recommend continuing the use of Nginx Ingress, cert-manager, and vouch-proxy, combining these services with automated DNS management via external-dns. This setup provides robustness, flexibility, and security, in harmony with Azure services.</p>"},{"location":"adr/016-networking.en-ca/#considered-alternatives","title":"Considered Alternatives","text":""},{"location":"adr/016-networking.en-ca/#istio-ingress","title":"Istio Ingress","text":"<p>Advantages:</p> <ul> <li> <p>Service Mesh Integration: Offers advanced management of routing, load balancing, and security policies at the service level.</p> </li> <li> <p>Granularity of Network Policies: Enables detailed management of network policies among services.</p> </li> </ul> <p>Disadvantages:</p> <ul> <li> <p>Complexity: Its configuration and management are complex and can pose challenges in terms of maintenance.</p> </li> <li> <p>Resource Consumption: Istio is more resource-intensive, which can be problematic in environments with limited capacity.</p> </li> </ul>"},{"location":"adr/016-networking.en-ca/#kubernetes-gateway-api","title":"Kubernetes Gateway API","text":"<p>Advantages:</p> <ul> <li> <p>Evolving Standard: Provides a more expressive model than traditional Ingress with better abstraction for routes.</p> </li> <li> <p>Modularity and Flexibility: Offers extensive customization in managing traffic and routes.</p> </li> </ul> <p>Disadvantages:</p> <ul> <li> <p>Maturity and Adoption: Given its novelty, the adoption of the Gateway API is not yet widespread and may lack support compared to established solutions.</p> </li> <li> <p>Possible Redundancy: Some capabilities may overlap with those provided by Nginx Ingress, potentially requiring a reevaluation of the tools in place.</p> </li> </ul>"},{"location":"adr/016-networking.en-ca/#conclusion","title":"Conclusion","text":"<p>The current configuration using Nginx Ingress, cert-manager, vouch-proxy, and external-dns for automated DNS record management provides an optimal balance between efficiency, security, and simplified management in our Azure environment. These choices support our strategy to maintain a stable architecture while fully leveraging the skills and technologies we are already proficient in. Continuing on this path allows us to benefit from strong integration and support by Azure, while facilitating the scalability and maintenance of our networking services.</p>"},{"location":"adr/016-networking.en-ca/#references","title":"References","text":"<ul> <li>Implementation for the Howard project</li> <li>Implementation of Vouch proxy for the Howard project</li> <li>Istio ingress</li> <li>Kubernetes gateway api</li> <li>Ingress NGINX</li> <li>Cert manager</li> <li>External DNS</li> </ul>"},{"location":"adr/017-security.en-ca/","title":"ADR-017: Security","text":""},{"location":"adr/017-security.en-ca/#executive-summary","title":"Executive Summary","text":"<p>This ADR documents and formalizes the enhancement of our security strategy through management and monitoring of vulnerabilities within our Kubernetes infrastructure. We have enhanced our security capabilities by integrating trivy-operator and Falco for static and dynamic scanning, while also continuing to use Dependabot for managing code dependencies. Additionally, we have adopted Mend Renovate to automate updates for Docker images, Terraform providers, and other dependencies to maintain the usage of the most secure versions. Monitoring and visualization of these security measures are conducted through Grafana dashboards.</p>"},{"location":"adr/017-security.en-ca/#background","title":"Background","text":"<p>Traditionally, our security strategy primarily relied on Dependabot for security alerts related to our code dependencies. In response to increased security demands and the need to address vulnerabilities both statically and dynamically, we expanded our suite of security tools.</p>"},{"location":"adr/017-security.en-ca/#decision","title":"Decision","text":"<p>We have bolstered our security architecture by incorporating the following tools:</p> <ul> <li> <p>Trivy-operator: Conducts static scanning of our cluster and the container images of our pods to detect vulnerabilities before deployment.</p> </li> <li> <p>Falco: Performs dynamic scanning to monitor real-time behavior of applications and detect any abnormal or suspicious activity.</p> </li> <li> <p>Mend Renovate: Automates the updates of our Docker images, Terraform providers, and other dependencies, ensuring the use of the latest and most secure versions.</p> </li> </ul> <p>These tools are complemented by Grafana dashboards, enabling real-time visualization and security alert management, facilitating swift and informed interventions.</p>"},{"location":"adr/017-security.en-ca/#considered-alternatives","title":"Considered Alternatives","text":""},{"location":"adr/017-security.en-ca/#snyk","title":"Snyk","text":"<p>Advantages:</p> <ul> <li> <p>Proactive Security: Snyk offers a proactive security solution by detecting vulnerabilities in dependencies and containers and proposing automated fixes.</p> </li> <li> <p>Wide Range of Integrations: Easily integrates with CI/CD tools and hosting platforms such as GitHub and Bitbucket, enabling automated vulnerability detection and remediation within the development pipeline.</p> </li> </ul> <p>Disadvantages:</p> <ul> <li> <p>Potentially High Cost: Advanced features of Snyk can be costly, particularly for large organizations managing numerous environments.</p> </li> <li> <p>Learning and Configuration: Setting up and optimizing Snyk for specific environments might require a significant learning curve and configuration time.</p> </li> </ul>"},{"location":"adr/017-security.en-ca/#sysdig-secure","title":"Sysdig Secure","text":"<p>Advantages:</p> <ul> <li> <p>Detailed Inspection: Provides in-depth analysis of real-time network traffic and file system monitoring.</p> </li> <li> <p>Wide Compatibility: Works with most cloud environments and container orchestrators.</p> </li> </ul> <p>Disadvantages:</p> <ul> <li> <p>Resource Intensive: May use a substantial amount of system resources, affecting performance.</p> </li> <li> <p>Complex Integration: Although powerful, it can be difficult to integrate and requires regular maintenance to stay updated with system and orchestrator updates.</p> </li> </ul>"},{"location":"adr/017-security.en-ca/#conclusion","title":"Conclusion","text":"<p>Integrating trivy-operator, Falco, and Mend Renovate, alongside Dependabot, significantly strengthens our security strategy, enabling us to proactively and in real time address vulnerabilities in our Kubernetes infrastructure. This comprehensive strategy, supported by clear visualization via Grafana, ensures that we maintain a solid security posture, which is crucial for the reliability and performance of our services.</p>"},{"location":"adr/017-security.en-ca/#references","title":"References","text":"<ul> <li>Falco</li> <li>Trivy</li> <li>MEND Renovate</li> <li>Dependabot</li> <li>Snyk</li> <li>Sysdig secure</li> </ul>"},{"location":"adr/018-monitoring.en-ca/","title":"ADR-018 : Observability Management","text":""},{"location":"adr/018-monitoring.en-ca/#executive-summary","title":"Executive Summary","text":"<p>We have decided to implement a comprehensive observability stack for our applications hosted on Azure Kubernetes Service (AKS) using OpenTelemetry, Grafana Alloy, and various Grafana components (Loki, Tempo, Prometheus). This decision aims to enhance our visibility into system performance, improve monitoring of critical services, and provide a vendor-neutral, open-source solution that aligns with our agency\u2019s requirements.</p> <p></p>"},{"location":"adr/018-monitoring.en-ca/#background","title":"Background","text":"<p>At the Canadian Food Inspection Agency, we faced significant challenges in monitoring and understanding the performance of our applications and system services on Azure Kubernetes Service (AKS). Despite achieving high availability and automation with AKS, we lacked contextualized data to diagnose issues and monitor the health of our nodes and services like ArgoCD and NGINX Ingress, as well as our client applications.</p> <p>We sought a solution that would provide centralized and open-source monitoring, offering insights into activity, traffic, and engagement with our applications. Our goal was to implement a generic, vendor-neutral solution for collecting and displaying telemetry data (logs, traces, and metrics) across our applications. After evaluating various options, we chose OpenTelemetry for data collection, Grafana Alloy as the collector, and Grafana\u2019s suite of tools (Loki, Tempo, Prometheus) for log, trace, and metric management.</p>"},{"location":"adr/018-monitoring.en-ca/#decision","title":"D\u00e9cision","text":""},{"location":"adr/018-monitoring.en-ca/#data-source-opentelemetry","title":"Data Source: OpenTelemetry","text":"<p>We chose OpenTelemetry as our primary data source for telemetry collection due to its vendor-neutral nature and consistent APIs/SDKs across multiple programming languages. This choice aligns well with our agency\u2019s diverse technological landscape and ensures broad compatibility and scalability.</p>"},{"location":"adr/018-monitoring.en-ca/#data-source-considered-alternatives","title":"Data Source: Considered alternatives","text":"<ol> <li>Jaeger:</li> <li>Advantages: Excellent tracing capabilities, integration with various      backend storage systems.</li> <li> <p>Disadvantages: Primarily focused on tracing, less support for metrics      and logs compared to OpenTelemetry.</p> </li> <li> <p>Zipkin:</p> </li> <li>Advantages: Simple and easy to deploy, good tracing support.</li> <li>Disadvantages: Limited support for metrics and logs, less flexibility      than OpenTelemetry.</li> </ol>"},{"location":"adr/018-monitoring.en-ca/#data-platform-grafana-alloy-loki-tempo-prometheus","title":"Data Platform: Grafana Alloy, Loki, Tempo, Prometheus","text":"<p>We decided to use Grafana Alloy as our OpenTelemetry collector, leveraging its vendor-neutral stance and seamless integration with other Grafana components. The primary allure of Grafana Alloy lies in its promise to provide a unified observability platform, akin to having a master key for various locks. This allows seamless integration with tools like Prometheus for metrics, Loki for logs, and many others. Alloy simplifies the observability stack, making it more accessible and manageable.</p> <p>For data sources, we selected Grafana Loki for logs, Grafana Tempo for traces, and Prometheus for metrics.</p> <ol> <li> <p>Alloy:</p> <ul> <li>Advantages: Unified observability platform, vendor-neutral, seamless integration with other Grafana components.</li> <li>Disadvantages: Complexity in setting up and managing large-scale log storage solutions.</li> </ul> </li> <li> <p>Loki:</p> <ul> <li>Advantages: Highly efficient log aggregation, scalable storage, and integration with Grafana for seamless visualization.</li> <li>Disadvantages: Complexity in setting up and managing large-scale log storage solutions.</li> </ul> </li> <li> <p>Tempo:</p> <ul> <li>Advantages: Efficient, scalable distributed tracing, integrates well with Grafana dashboards.</li> <li>Disadvantages: Can be resource-intensive, requires proper configuration for optimal performance.</li> </ul> </li> <li> <p>Prometheus:</p> <ul> <li>Advantages: Robust metric collection and querying capabilities, strong ecosystem support, and alert management.</li> <li>Disadvantages: Can be resource-intensive, particularly at scale, and requires significant setup and maintenance.</li> </ul> </li> </ol>"},{"location":"adr/018-monitoring.en-ca/#data-platform-considered-alternatives","title":"Data Platform: considered alternatives","text":"<ol> <li> <p>Native OpenTelemetry collector:</p> <ul> <li>Advantages:<ul> <li>Direct integration: Direct support for OpenTelemetry protocols ensures seamless data collection and export.</li> <li>Highly customizable: Provides extensive customization options to tailor the observability pipeline to specific needs.</li> </ul> </li> <li>Disadvantages:<ul> <li>Complex configuration: Requires significant configuration and setup effort, which can be time-consuming.</li> <li>Lack of integrated dashboards: Does not offer built-in visualization tools, requiring additional integrations for dashboards.</li> </ul> </li> </ul> </li> <li> <p>Azure Monitor:</p> <ul> <li>Advantages:<ul> <li>Native Azure integration: Seamless integration with Azure services and resources, simplifying setup and management.</li> <li>Comprehensive monitoring: Offers a wide range of monitoring and observability features, including logs, metrics, and traces.</li> </ul> </li> <li>Disadvantages:<ul> <li>Vendor lock-in: Tied to the Azure ecosystem, limiting flexibility and portability.</li> <li>Costly at scale: Costs can escalate quickly, especially with large volumes of data.</li> <li>Configuration limitations: Limited ability to configure as code,   potentially hindering automation and version control efforts.</li> <li>Limited community support: Relatively limited community compared to open-source solutions, potentially limiting resources and support.</li> </ul> </li> </ul> </li> <li> <p>ClickHouse</p> <ul> <li>Advantages:<ul> <li>High performance: Exceptional performance in handling large volumes of data, suitable for intensive analytics.</li> <li>Scalable: Scales efficiently to handle growing data needs without significant performance degradation.</li> </ul> </li> <li>Disadvantages:<ul> <li>Complex setup: Requires a complex setup and configuration process, making it challenging for smaller teams.</li> <li>Additional tooling needed: Needs additional tools for visualization and integration, increasing the overall complexity of the stack.</li> </ul> </li> </ul> </li> <li> <p>Elasticsearch</p> <ul> <li>Advantages:<ul> <li>Powerful search capabilities: Offers robust search and analytics capabilities, making it ideal for log management.</li> <li>Wide adoption: Widely adopted with a large community, providing extensive resources and support.</li> </ul> </li> <li>Disadvantages:<ul> <li>Resource-intensive: Can be resource-intensive, requiring substantial infrastructure to run efficiently.</li> <li>Costly at scale: Costs can escalate quickly, especially when dealing with large volumes of data.</li> </ul> </li> </ul> </li> </ol>"},{"location":"adr/018-monitoring.en-ca/#consumption-grafana","title":"Consumption: Grafana","text":"<p>For data visualization and dashboarding, we chose Grafana due to its robust support for diverse data sources, extensive community, and powerful dashboard capabilities. Grafana allows us to create custom dashboards to monitor various aspects of our system, including application activity, traffic, and performance metrics.</p>"},{"location":"adr/018-monitoring.en-ca/#consumption-considered-alternatives","title":"Consumption: Considered alternatives","text":"<ol> <li>Kibana:</li> <li>Advantages: Excellent for visualizing data stored in Elasticsearch,      powerful search and analytics capabilities.</li> <li> <p>Disadvantages: Primarily tied to the Elasticsearch ecosystem, less      flexibility with other data sources.</p> </li> <li> <p>Azure Log Analytics:</p> </li> <li>Advantages: Native integration with Azure services, comprehensive      monitoring capabilities.</li> <li> <p>Disadvantages: Vendor lock-in, limited support for non-Azure data      sources.</p> </li> <li> <p>DataDog:</p> </li> <li>Advantages: Comprehensive monitoring and observability platform, strong      integrations.</li> <li> <p>Disadvantages: Proprietary, can be expensive, less flexibility for      open-source integrations.</p> </li> <li> <p>Prometheus UI:</p> </li> <li>Advantages: Native support for Prometheus metrics, simple to use.</li> <li>Disadvantages: Limited visualization capabilities compared to Grafana,      primarily focused on metrics.</li> </ol>"},{"location":"adr/018-monitoring.en-ca/#conclusion","title":"Conclusion","text":"<p>Implementing OpenTelemetry for data collection, Grafana Alloy for data processing, and Grafana for visualization provides a fully open-source, vendor-neutral observability stack. This approach enhances our ability to monitor and troubleshoot our applications and services effectively. The chosen solutions offer scalability, flexibility, and robust community support, ensuring we can meet our current and future observability needs. Furthermore, OpenTelemetry is a widely adopted standard in the market, empowering our developers to work with industry norms. Since we are in the early development stages of our prototypes, this is a perfect opportunity to provide instrumentation by design and ensure visibility over performance from the outset. This proactive approach will help us optimize our applications' performance and reliability in the early stages of development.</p>"},{"location":"adr/018-monitoring.en-ca/#references","title":"References","text":"<ul> <li>Microsoft Azure Monitor and   OpenTelemetry</li> <li>Grafana Alloy</li> <li>Grafana Loki</li> <li>Grafana Tempo</li> <li>Prometheus</li> <li>OpenTelemetry documentation</li> <li>Jaeger</li> <li>Zipkin</li> <li>ClickHouse</li> <li>Elasticsearch</li> <li>Kibana</li> <li>DataDog Documentation</li> <li>Prometheus UI</li> </ul>"}]}